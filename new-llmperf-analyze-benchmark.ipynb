{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0da7dc-cb1b-455e-a592-bc02798cb02c",
   "metadata": {},
   "source": [
    "### Explanation of the Script\n",
    "\n",
    "- **Intended Use**: This script is designed to process and analyze benchmark results stored in multiple `summary.json` files. It extracts key metrics related to the performance of large language models (LLMs) and generates visualizations to help users interpret the results.\n",
    "  \n",
    "- **Setup**: \n",
    "  - Specify the directory where your benchmark results are located by updating the `benchmark_dir` variable.\n",
    "  - The script will automatically search for all `summary.json` files within this directory (and subdirectories).\n",
    "\n",
    "- **Data Extraction**: \n",
    "  - The script will read each `summary.json` file, extracting key metrics such as:\n",
    "    - Inter-Token Latency (P90)\n",
    "    - Time to First Token (TTFT) Latency (P90)\n",
    "    - End-to-End Latency (P90)\n",
    "    - Request Output Throughput (P90)\n",
    "    - Mean Output Throughput\n",
    "    - **Concurrent Requests**: The number of concurrent requests made during the benchmark run.\n",
    "  - These metrics are stored in a pandas DataFrame for further analysis.\n",
    "\n",
    "- **Data Visualization**:\n",
    "  - The script generates plots showing how these metrics change based on the number of concurrent requests.\n",
    "  - Each plot compares different token combinations (input/output) across concurrent requests.\n",
    "  - It uses different colors, markers, and line styles for easy distinction between token combinations.\n",
    "  \n",
    "- **Saving Results**:\n",
    "  - Plots are automatically saved as images in the `plots` directory inside the benchmark folder.\n",
    "  - The aggregated data from the summary files is saved into a CSV file (`aggregated_results.csv`) for further analysis.\n",
    "\n",
    "- **Error Handling**: \n",
    "  - The script includes basic error handling to manage issues like missing files or incorrect JSON formats, ensuring the process continues without crashing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3450c73e-600a-4e37-bf40-23ddd6364ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benchmark_dir = 'llmperf/vllm_bench_results/tp32-b18/'\n",
    "\n",
    "# Install necessary libraries\n",
    "!pip install pandas matplotlib\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Validate the directory\n",
    "if not os.path.exists(benchmark_dir):\n",
    "    print(f\"The directory {benchmark_dir} does not exist. Please check the path and try again.\")\n",
    "else:\n",
    "    print(f\"Benchmark results directory set to: {benchmark_dir}\")\n",
    "\n",
    "    # Option to save plots\n",
    "    save_plots = True  # Set to False if you don't want to save the plots as images\n",
    "\n",
    "    # Create a directory to save plots if it doesn't exist\n",
    "    plots_dir = os.path.join(benchmark_dir, 'plots')\n",
    "    if save_plots:\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "        print(f\"Plots will be saved to: {plots_dir}\")\n",
    "\n",
    "    # Search for summary.json files\n",
    "    summary_files = glob.glob(os.path.join(benchmark_dir, '**', '*summary.json'), recursive=True)\n",
    "\n",
    "    print(f\"Found {len(summary_files)} summary files.\")\n",
    "\n",
    "    # Initialize a list to store data\n",
    "    data_list = []\n",
    "\n",
    "    # Extract data from each summary file\n",
    "    for summary_file in summary_files:\n",
    "        try:\n",
    "            with open(summary_file, 'r') as f:\n",
    "                summary_data = json.load(f)\n",
    "\n",
    "                data = {\n",
    "                    'mean_input_tokens': summary_data.get('mean_input_tokens'),\n",
    "                    'mean_output_tokens': summary_data.get('mean_output_tokens'),\n",
    "                    'num_concurrent_requests': summary_data.get('num_concurrent_requests'),\n",
    "                    'results_inter_token_latency_s_quantiles_p90': summary_data.get('results_inter_token_latency_s_quantiles_p90'),\n",
    "                    'results_ttft_s_quantiles_p90': summary_data.get('results_ttft_s_quantiles_p90'),\n",
    "                    'results_end_to_end_latency_s_quantiles_p90': summary_data.get('results_end_to_end_latency_s_quantiles_p90'),\n",
    "                    'results_request_output_throughput_token_per_s_quantiles_p90': summary_data.get('results_request_output_throughput_token_per_s_quantiles_p90'),\n",
    "                    'results_mean_output_throughput_token_per_s': summary_data.get('results_mean_output_throughput_token_per_s')\n",
    "                }\n",
    "\n",
    "                data_list.append(data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from file {summary_file}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing file {summary_file}: {e}\")\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    # Define numeric columns\n",
    "    numeric_columns = [\n",
    "        'mean_input_tokens',\n",
    "        'mean_output_tokens',\n",
    "        'num_concurrent_requests',\n",
    "        'results_inter_token_latency_s_quantiles_p90',\n",
    "        'results_ttft_s_quantiles_p90',\n",
    "        'results_end_to_end_latency_s_quantiles_p90',\n",
    "        'results_request_output_throughput_token_per_s_quantiles_p90',\n",
    "        'results_mean_output_throughput_token_per_s'\n",
    "    ]\n",
    "\n",
    "    # Convert columns to numeric types\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Drop rows with NaN values in key metrics\n",
    "    df = df.dropna(subset=['mean_input_tokens', 'mean_output_tokens', 'num_concurrent_requests'])\n",
    "\n",
    "    # Sort the DataFrame\n",
    "    df = df.sort_values(['mean_input_tokens', 'mean_output_tokens', 'num_concurrent_requests'])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Aggregate data if necessary\n",
    "    aggregated_df = df.groupby(\n",
    "        ['mean_input_tokens', 'mean_output_tokens', 'num_concurrent_requests'],\n",
    "        as_index=False\n",
    "    ).mean()\n",
    "\n",
    "    # Get unique token combinations\n",
    "    token_combinations = aggregated_df[['mean_input_tokens', 'mean_output_tokens']].drop_duplicates()\n",
    "\n",
    "    # Define the metrics to plot\n",
    "    metrics = {\n",
    "        'results_inter_token_latency_s_quantiles_p90': {\n",
    "            'ylabel': 'Inter-Token Latency P90 (s)'\n",
    "        },\n",
    "        'results_ttft_s_quantiles_p90': {\n",
    "            'ylabel': 'TTFT P90 (s)'\n",
    "        },\n",
    "        'results_end_to_end_latency_s_quantiles_p90': {\n",
    "            'ylabel': 'End-to-End Latency P90 (s)'\n",
    "        },\n",
    "        'results_request_output_throughput_token_per_s_quantiles_p90': {\n",
    "            'ylabel': 'Request Output Throughput P90 (tokens/s)'\n",
    "        },\n",
    "        'results_mean_output_throughput_token_per_s': {\n",
    "            'ylabel': 'Mean Output Throughput (tokens/s)'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define colors, markers, and line styles\n",
    "    colors = ['blue', 'green', 'red', 'purple', 'orange', 'cyan', 'magenta', 'black', 'brown']\n",
    "    markers = ['o', 's', '^', 'D', 'v', '*', 'P', 'X', 'h']\n",
    "    line_styles = ['-', '--', '-.', ':']\n",
    "\n",
    "    # Loop over each metric to plot\n",
    "    for metric, info in metrics.items():\n",
    "        ylabel = info['ylabel']\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for idx, (_, row) in enumerate(token_combinations.iterrows()):\n",
    "            input_tokens = row['mean_input_tokens']\n",
    "            output_tokens = row['mean_output_tokens']\n",
    "            subset = aggregated_df[\n",
    "                (aggregated_df['mean_input_tokens'] == input_tokens) &\n",
    "                (aggregated_df['mean_output_tokens'] == output_tokens)\n",
    "            ].sort_values('num_concurrent_requests')\n",
    "            if subset.empty:\n",
    "                continue\n",
    "            color = colors[idx % len(colors)]\n",
    "            marker = markers[idx % len(markers)]\n",
    "            line_style = line_styles[idx % len(line_styles)]\n",
    "            plt.plot(\n",
    "                subset['num_concurrent_requests'],\n",
    "                subset[metric],\n",
    "                marker=marker,\n",
    "                color=color,\n",
    "                linestyle=line_style,\n",
    "                label=f\"Input: {input_tokens}, Output: {output_tokens}\"\n",
    "            )\n",
    "        plt.title(f'{ylabel} vs. Number of Concurrent Requests')\n",
    "        plt.xlabel('Number of Concurrent Requests')\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xticks(sorted(aggregated_df['num_concurrent_requests'].unique()))\n",
    "        plt.legend(title='Token Counts')\n",
    "        plt.grid(True)\n",
    "        if save_plots:\n",
    "            # Construct a safe filename\n",
    "            safe_metric_name = metric.replace(' ', '_').replace('/', '_per_')\n",
    "            plot_filename = f\"{safe_metric_name}_vs_concurrent_requests.png\"\n",
    "            plot_path = os.path.join(plots_dir, plot_filename)\n",
    "            plt.savefig(plot_path)\n",
    "            print(f\"Plot saved to: {plot_path}\")\n",
    "        plt.show()\n",
    "\n",
    "    # Optional: Save aggregated data to CSV\n",
    "    output_csv = os.path.join(benchmark_dir, 'aggregated_results.csv')\n",
    "    aggregated_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Aggregated data saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd1813-688d-48d6-a24f-d5f2da4a4bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
