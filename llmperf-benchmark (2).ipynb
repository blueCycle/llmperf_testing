{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b10c8c-bd12-4ace-9124-c2bc30c297aa",
   "metadata": {},
   "source": [
    "### Explanation of the Script\n",
    "\n",
    "- **Purpose**: This script is designed to automate the benchmarking of a large language model (LLM) hosted on a remote server using LLMPerf. It runs performance tests across varying numbers of concurrent requests and stores the results for further analysis.\n",
    "\n",
    "- **Setup**:\n",
    "  - You can modify key parameters such as:\n",
    "    - **Max Concurrent Requests**: Number of concurrent requests to test (default is 8).\n",
    "    - **Remote Server IP**: IP address where the LLM API is hosted.\n",
    "    - **Model**: Specify the model to use for the benchmark (default: `meta-llama/Meta-Llama-3.1-8B-Instruct`).\n",
    "    - **Input/Output Token Distribution**: Define the mean and standard deviation for input and output tokens to simulate realistic request sizes.\n",
    "\n",
    "- **Repository Setup**:\n",
    "  - The script clones the `llmperf` repository (if it doesnâ€™t already exist) to the specified directory. This repository contains the necessary benchmarking tools.\n",
    "  - The working directory is then switched to the `llmperf` directory for execution.\n",
    "\n",
    "- **Environment Configuration**:\n",
    "  - The script sets up the necessary environment variables for interacting with the LLM API.\n",
    "  - The OpenAI API base is dynamically set to point to the specified remote IP.\n",
    "\n",
    "- **Benchmark Execution**:\n",
    "  - The script runs benchmarks in a loop, starting from 1 concurrent request and increasing up to the specified `max_concurrent_requests`.\n",
    "  - For each iteration:\n",
    "    - A results directory is created with a timestamp to store the output of each run.\n",
    "    - The script constructs and prints a custom benchmark command to execute the benchmarking tool for the current number of concurrent requests.\n",
    "    - The benchmark is run by executing the generated command.\n",
    "\n",
    "- **Results**:\n",
    "  - After running all benchmarks, the script lists all files and directories inside the base results directory, allowing you to see all results from each benchmark run.\n",
    "  - The results are saved in a structured manner within subdirectories, categorized by the number of concurrent requests.\n",
    "\n",
    "- **Customizable Parameters**:\n",
    "  - `max_concurrent_requests`: Maximum concurrent requests to test.\n",
    "  - `remote_ip`: IP address of the remote server where the LLM is hosted.\n",
    "  - `model`: LLM model name for benchmarking.\n",
    "  - `mean_input_tokens` and `mean_output_tokens`: Define token distributions for input and output sizes.\n",
    "  - `timeout`: Maximum duration for each benchmark run.\n",
    "  - **Destination Folder for Results**: The `base_results_dir` specifies the directory where all benchmark results will be stored. This path is timestamped and organized by concurrent request numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cece1d-2661-4690-af2e-6fb3c2f4e641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# User Variables - Modify these as needed\n",
    "\n",
    "# Maximum number of concurrent requests to test\n",
    "max_concurrent_requests = 18\n",
    "\n",
    "# IP address of the remote server\n",
    "remote_ip = 'x.x.x.x'\n",
    "\n",
    "# Model to use for benchmarking\n",
    "model = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "\n",
    "# Mean and standard deviation of input tokens\n",
    "mean_input_tokens = 6000\n",
    "stddev_input_tokens = 200\n",
    "\n",
    "# Mean and standard deviation of output tokens\n",
    "mean_output_tokens = 150\n",
    "stddev_output_tokens = 50\n",
    "\n",
    "# Absolute path to clone the llmperf repository\n",
    "llmperf_dir = '/home/ec2-user/SageMaker/llmperf'\n",
    "\n",
    "# Maximum time to wait for each benchmark run (in seconds)\n",
    "timeout = 7200\n",
    "\n",
    "# Additional sampling parameters for the benchmark\n",
    "additional_sampling_params = '{}'\n",
    "\n",
    "# Clone the llmperf repository if it doesn't exist\n",
    "if not os.path.exists(llmperf_dir):\n",
    "    !git clone https://github.com/ray-project/llmperf.git {llmperf_dir}\n",
    "else:\n",
    "    print(f\"'llmperf' directory already exists at {llmperf_dir}\")\n",
    "\n",
    "# Change the current working directory to the llmperf directory\n",
    "os.chdir(llmperf_dir)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Install required packages\n",
    "!pip install setuptools==65.5.0\n",
    "!pip install -e .\n",
    "\n",
    "# Set environment variables for the OpenAI API\n",
    "os.environ['OPENAI_API_KEY'] = 'EMPTY'\n",
    "os.environ['OPENAI_API_BASE'] = f\"http://{remote_ip}:8000/v1\"\n",
    "\n",
    "# Generate a timestamped directory name for results\n",
    "date_str = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "# Modify this to change the results directory\n",
    "base_results_dir = f\"vllm_bench_results/tp32-b18/{date_str}\"\n",
    "\n",
    "print(f\"Base Results Directory: {base_results_dir}\")\n",
    "\n",
    "# Create the base results directory\n",
    "os.makedirs(base_results_dir, exist_ok=True)\n",
    "\n",
    "# Run benchmarks for concurrent requests from 1 to max_concurrent_requests\n",
    "for concurrent_requests in range(1, max_concurrent_requests + 1):\n",
    "    print(f\"\\nRunning benchmark with {concurrent_requests} concurrent requests...\")\n",
    "    \n",
    "    # Compute the maximum number of requests for this run\n",
    "    max_requests = concurrent_requests * 1\n",
    "    \n",
    "    # Create a subdirectory for this run's results\n",
    "    results_dir = os.path.join(base_results_dir, f\"concurrent_{concurrent_requests}\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    print(f\"Results Directory: {results_dir}\")\n",
    "    \n",
    "    # Construct the benchmark command\n",
    "    benchmark_command = f\"\"\"\n",
    "    python3 ./token_benchmark_ray.py \\\\\n",
    "     --model {model} \\\\\n",
    "     --mean-input-tokens {mean_input_tokens} \\\\\n",
    "     --stddev-input-tokens {stddev_input_tokens} \\\\\n",
    "     --mean-output-tokens {mean_output_tokens} \\\\\n",
    "     --stddev-output-tokens {stddev_output_tokens} \\\\\n",
    "     --max-num-completed-requests {max_requests} \\\\\n",
    "     --timeout {timeout} \\\\\n",
    "     --num-concurrent-requests {concurrent_requests} \\\\\n",
    "     --results-dir \"{results_dir}\" \\\\\n",
    "     --llm-api openai \\\\\n",
    "     --additional-sampling-params '{additional_sampling_params}'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Print the command for verification\n",
    "    print(\"Executing Benchmark Command:\")\n",
    "    print(benchmark_command)\n",
    "    \n",
    "    # Execute the benchmark command\n",
    "    !{benchmark_command}\n",
    "    \n",
    "# List the contents of the base results directory\n",
    "print(\"\\nAll Benchmark Results:\")\n",
    "!ls -l \"{base_results_dir}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721b768-ad73-4c0d-8134-e23cf92fe517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
