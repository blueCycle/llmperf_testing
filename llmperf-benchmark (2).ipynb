{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b10c8c-bd12-4ace-9124-c2bc30c297aa",
   "metadata": {},
   "source": [
    "### Explanation of the Script\n",
    "\n",
    "- **Purpose**: This script is designed to automate the benchmarking of a large language model (LLM) hosted on a remote server using LLMPerf. It runs performance tests across varying numbers of concurrent requests and stores the results for further analysis.\n",
    "\n",
    "- **Setup**:\n",
    "  - You can modify key parameters such as:\n",
    "    - **Max Concurrent Requests**: Number of concurrent requests to test (default is 8).\n",
    "    - **Remote Server IP**: IP address where the LLM API is hosted.\n",
    "    - **Model**: Specify the model to use for the benchmark (default: `meta-llama/Meta-Llama-3.1-8B-Instruct`).\n",
    "    - **Input/Output Token Distribution**: Define the mean and standard deviation for input and output tokens to simulate realistic request sizes.\n",
    "\n",
    "- **Repository Setup**:\n",
    "  - The script clones the `llmperf` repository (if it doesn’t already exist) to the specified directory. This repository contains the necessary benchmarking tools.\n",
    "  - The working directory is then switched to the `llmperf` directory for execution.\n",
    "\n",
    "- **Environment Configuration**:\n",
    "  - The script sets up the necessary environment variables for interacting with the LLM API.\n",
    "  - The OpenAI API base is dynamically set to point to the specified remote IP.\n",
    "\n",
    "- **Benchmark Execution**:\n",
    "  - The script runs benchmarks in a loop, starting from 1 concurrent request and increasing up to the specified `max_concurrent_requests`.\n",
    "  - For each iteration:\n",
    "    - A results directory is created with a timestamp to store the output of each run.\n",
    "    - The script constructs and prints a custom benchmark command to execute the benchmarking tool for the current number of concurrent requests.\n",
    "    - The benchmark is run by executing the generated command.\n",
    "\n",
    "- **Results**:\n",
    "  - After running all benchmarks, the script lists all files and directories inside the base results directory, allowing you to see all results from each benchmark run.\n",
    "  - The results are saved in a structured manner within subdirectories, categorized by the number of concurrent requests.\n",
    "\n",
    "- **Customizable Parameters**:\n",
    "  - `max_concurrent_requests`: Maximum concurrent requests to test.\n",
    "  - `remote_ip`: IP address of the remote server where the LLM is hosted.\n",
    "  - `model`: LLM model name for benchmarking.\n",
    "  - `mean_input_tokens` and `mean_output_tokens`: Define token distributions for input and output sizes.\n",
    "  - `timeout`: Maximum duration for each benchmark run.\n",
    "  - **Destination Folder for Results**: The `base_results_dir` specifies the directory where all benchmark results will be stored. This path is timestamped and organized by concurrent request numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62cece1d-2661-4690-af2e-6fb3c2f4e641",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'llmperf' directory already exists at /home/ec2-user/SageMaker/llmperf\n",
      "Current working directory: /home/ec2-user/SageMaker/llmperf\n",
      "Requirement already satisfied: setuptools==65.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (65.5.0)\n",
      "Obtaining file:///home/ec2-user/SageMaker/llmperf\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pydantic<2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from LLMPerf==0.1.0) (2.4.2)\n",
      "Requirement already satisfied: ray in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from LLMPerf==0.1.0) (2.38.0)\n",
      "Requirement already satisfied: pytest>=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from LLMPerf==0.1.0) (8.3.3)\n",
      "Requirement already satisfied: seaborn>=0.11 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from LLMPerf==0.1.0) (0.13.2)\n",
      "Requirement already satisfied: awscli>=1.22 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from LLMPerf==0.1.0) (1.35.9)\n",
      "Requirement already satisfied: typer>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from LLMPerf==0.1.0) (0.12.5)\n",
      "Requirement already satisfied: litellm>=0.1.738 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from LLMPerf==0.1.0) (1.50.4)\n",
      "Requirement already satisfied: num2words in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from LLMPerf==0.1.0) (0.5.13)\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from LLMPerf==0.1.0) (4.46.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from LLMPerf==0.1.0) (4.66.5)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from LLMPerf==0.1.0) (1.35.43)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from LLMPerf==0.1.0) (1.70.0)\n",
      "Requirement already satisfied: botocore==1.35.43 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from awscli>=1.22->LLMPerf==0.1.0) (1.35.43)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from awscli>=1.22->LLMPerf==0.1.0) (0.16)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from awscli>=1.22->LLMPerf==0.1.0) (0.10.2)\n",
      "Requirement already satisfied: PyYAML<6.1,>=3.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from awscli>=1.22->LLMPerf==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: colorama<0.4.7,>=0.2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from awscli>=1.22->LLMPerf==0.1.0) (0.4.6)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from awscli>=1.22->LLMPerf==0.1.0) (4.7.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore==1.35.43->awscli>=1.22->LLMPerf==0.1.0) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore==1.35.43->awscli>=1.22->LLMPerf==0.1.0) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore==1.35.43->awscli>=1.22->LLMPerf==0.1.0) (2.2.3)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (3.10.10)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (6.11.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.52.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (1.52.2)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (1.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (2.32.3)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (0.8.0)\n",
      "Requirement already satisfied: tokenizers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from litellm>=0.1.738->LLMPerf==0.1.0) (0.20.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<2.5->LLMPerf==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<2.5->LLMPerf==0.1.0) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<2.5->LLMPerf==0.1.0) (4.12.2)\n",
      "Requirement already satisfied: iniconfig in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pytest>=6.0->LLMPerf==0.1.0) (2.0.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pytest>=6.0->LLMPerf==0.1.0) (21.3)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pytest>=6.0->LLMPerf==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pytest>=6.0->LLMPerf==0.1.0) (1.2.2)\n",
      "Requirement already satisfied: tomli>=1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pytest>=6.0->LLMPerf==0.1.0) (2.0.1)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from seaborn>=0.11->LLMPerf==0.1.0) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from seaborn>=0.11->LLMPerf==0.1.0) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from seaborn>=0.11->LLMPerf==0.1.0) (3.9.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from typer>=0.4->LLMPerf==0.1.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from typer>=0.4->LLMPerf==0.1.0) (13.8.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->LLMPerf==0.1.0) (2.21.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (2.35.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (1.25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (5.28.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (2.18.2)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (3.26.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (1.12.5)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (2.0.6)\n",
      "Requirement already satisfied: docstring-parser<1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-cloud-aiplatform->LLMPerf==0.1.0) (0.16)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from num2words->LLMPerf==0.1.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray->LLMPerf==0.1.0) (3.16.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray->LLMPerf==0.1.0) (1.1.0)\n",
      "Requirement already satisfied: aiosignal in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray->LLMPerf==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ray->LLMPerf==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers->LLMPerf==0.1.0) (0.26.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers->LLMPerf==0.1.0) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers->LLMPerf==0.1.0) (0.4.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->LLMPerf==0.1.0) (1.65.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->LLMPerf==0.1.0) (1.67.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->LLMPerf==0.1.0) (1.67.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform->LLMPerf==0.1.0) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform->LLMPerf==0.1.0) (0.4.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=2.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform->LLMPerf==0.1.0) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform->LLMPerf==0.1.0) (2.7.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform->LLMPerf==0.1.0) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform->LLMPerf==0.1.0) (1.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers->LLMPerf==0.1.0) (2024.9.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata>=6.8.0->litellm>=0.1.738->LLMPerf==0.1.0) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=0.1.738->LLMPerf==0.1.0) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=0.1.738->LLMPerf==0.1.0) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=0.1.738->LLMPerf==0.1.0) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=0.1.738->LLMPerf==0.1.0) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=0.1.738->LLMPerf==0.1.0) (0.20.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11->LLMPerf==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11->LLMPerf==0.1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11->LLMPerf==0.1.0) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11->LLMPerf==0.1.0) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11->LLMPerf==0.1.0) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.11->LLMPerf==0.1.0) (3.1.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openai>=1.52.0->litellm>=0.1.738->LLMPerf==0.1.0) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openai>=1.52.0->litellm>=0.1.738->LLMPerf==0.1.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openai>=1.52.0->litellm>=0.1.738->LLMPerf==0.1.0) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openai>=1.52.0->litellm>=0.1.738->LLMPerf==0.1.0) (0.6.1)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openai>=1.52.0->litellm>=0.1.738->LLMPerf==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=1.2->seaborn>=0.11->LLMPerf==0.1.0) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->litellm>=0.1.738->LLMPerf==0.1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->litellm>=0.1.738->LLMPerf==0.1.0) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->litellm>=0.1.738->LLMPerf==0.1.0) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.4->LLMPerf==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.4->LLMPerf==0.1.0) (2.18.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rsa<4.8,>=3.1.2->awscli>=1.22->LLMPerf==0.1.0) (0.6.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->litellm>=0.1.738->LLMPerf==0.1.0) (2.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->litellm>=0.1.738->LLMPerf==0.1.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->litellm>=0.1.738->LLMPerf==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->litellm>=0.1.738->LLMPerf==0.1.0) (4.0.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.52.0->litellm>=0.1.738->LLMPerf==0.1.0) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.52.0->litellm>=0.1.738->LLMPerf==0.1.0) (0.14.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.4->LLMPerf==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.35.43->awscli>=1.22->LLMPerf==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->litellm>=0.1.738->LLMPerf==0.1.0) (0.2.0)\n",
      "Building wheels for collected packages: LLMPerf\n",
      "  Building editable for LLMPerf (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for LLMPerf: filename=LLMPerf-0.1.0-0.editable-py3-none-any.whl size=6128 sha256=e3e03d9293781e6eb8acfe24c08bd77381cafae9a9cc702f5d98af7685ff0b82\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wjzu345b/wheels/a4/f3/12/0a40b272a917334cd75a7dd0ecdbed82654f530ba110b5a320\n",
      "Successfully built LLMPerf\n",
      "Installing collected packages: LLMPerf\n",
      "  Attempting uninstall: LLMPerf\n",
      "    Found existing installation: LLMPerf 0.1.0\n",
      "    Uninstalling LLMPerf-0.1.0:\n",
      "      Successfully uninstalled LLMPerf-0.1.0\n",
      "Successfully installed LLMPerf-0.1.0\n",
      "Base Results Directory: vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16\n",
      "\n",
      "Running benchmark with 1 concurrent requests...\n",
      "Results Directory: vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_1\n",
      "Executing Benchmark Command:\n",
      "\n",
      "    python3 ./token_benchmark_ray.py \\\n",
      "     --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
      "     --mean-input-tokens 6000 \\\n",
      "     --stddev-input-tokens 200 \\\n",
      "     --mean-output-tokens 150 \\\n",
      "     --stddev-output-tokens 50 \\\n",
      "     --max-num-completed-requests 1 \\\n",
      "     --timeout 7200 \\\n",
      "     --num-concurrent-requests 1 \\\n",
      "     --results-dir \"vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_1\" \\\n",
      "     --llm-api openai \\\n",
      "     --additional-sampling-params '{}'\n",
      "    \n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "2024-11-20 22:56:20,582\tINFO worker.py:1816 -- Started a local Ray instance.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.19s/it]\n",
      "\\Results for token benchmark for meta-llama/Meta-Llama-3.1-8B-Instruct queried with the openai api.\n",
      "\n",
      "inter_token_latency_s\n",
      "    p25 = 0.017345448483492395\n",
      "    p50 = 0.017345448483492395\n",
      "    p75 = 0.017345448483492395\n",
      "    p90 = 0.017345448483492395\n",
      "    p95 = 0.017345448483492395\n",
      "    p99 = 0.017345448483492395\n",
      "    mean = 0.017345448483492395\n",
      "    min = 0.017345448483492395\n",
      "    max = 0.017345448483492395\n",
      "    stddev = nan\n",
      "ttft_s\n",
      "    p25 = 0.5350309829809703\n",
      "    p50 = 0.5350309829809703\n",
      "    p75 = 0.5350309829809703\n",
      "    p90 = 0.5350309829809703\n",
      "    p95 = 0.5350309829809703\n",
      "    p99 = 0.5350309829809703\n",
      "    mean = 0.5350309829809703\n",
      "    min = 0.5350309829809703\n",
      "    max = 0.5350309829809703\n",
      "    stddev = nan\n",
      "end_to_end_latency_s\n",
      "    p25 = 3.2960078659816645\n",
      "    p50 = 3.2960078659816645\n",
      "    p75 = 3.2960078659816645\n",
      "    p90 = 3.2960078659816645\n",
      "    p95 = 3.2960078659816645\n",
      "    p99 = 3.2960078659816645\n",
      "    mean = 3.2960078659816645\n",
      "    min = 3.2960078659816645\n",
      "    max = 3.2960078659816645\n",
      "    stddev = nan\n",
      "request_output_throughput_token_per_s\n",
      "    p25 = 57.64549349563262\n",
      "    p50 = 57.64549349563262\n",
      "    p75 = 57.64549349563262\n",
      "    p90 = 57.64549349563262\n",
      "    p95 = 57.64549349563262\n",
      "    p99 = 57.64549349563262\n",
      "    mean = 57.64549349563262\n",
      "    min = 57.64549349563262\n",
      "    max = 57.64549349563262\n",
      "    stddev = nan\n",
      "number_input_tokens\n",
      "    p25 = 5985.0\n",
      "    p50 = 5985.0\n",
      "    p75 = 5985.0\n",
      "    p90 = 5985.0\n",
      "    p95 = 5985.0\n",
      "    p99 = 5985.0\n",
      "    mean = 5985.0\n",
      "    min = 5985\n",
      "    max = 5985\n",
      "    stddev = nan\n",
      "number_output_tokens\n",
      "    p25 = 190.0\n",
      "    p50 = 190.0\n",
      "    p75 = 190.0\n",
      "    p90 = 190.0\n",
      "    p95 = 190.0\n",
      "    p99 = 190.0\n",
      "    mean = 190.0\n",
      "    min = 190\n",
      "    max = 190\n",
      "    stddev = nan\n",
      "Number Of Errored Requests: 0\n",
      "Overall Output Throughput: 45.276476063149275\n",
      "Number Of Completed Requests: 1\n",
      "Completed Requests Per Minute: 14.297834546257665\n",
      "\u001b[0m\n",
      "Running benchmark with 2 concurrent requests...\n",
      "Results Directory: vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_2\n",
      "Executing Benchmark Command:\n",
      "\n",
      "    python3 ./token_benchmark_ray.py \\\n",
      "     --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
      "     --mean-input-tokens 6000 \\\n",
      "     --stddev-input-tokens 200 \\\n",
      "     --mean-output-tokens 150 \\\n",
      "     --stddev-output-tokens 50 \\\n",
      "     --max-num-completed-requests 2 \\\n",
      "     --timeout 7200 \\\n",
      "     --num-concurrent-requests 2 \\\n",
      "     --results-dir \"vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_2\" \\\n",
      "     --llm-api openai \\\n",
      "     --additional-sampling-params '{}'\n",
      "    \n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "2024-11-20 22:56:32,372\tINFO worker.py:1816 -- Started a local Ray instance.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:04<00:00,  2.29s/it]\n",
      "\\Results for token benchmark for meta-llama/Meta-Llama-3.1-8B-Instruct queried with the openai api.\n",
      "\n",
      "inter_token_latency_s\n",
      "    p25 = 0.019619537219161647\n",
      "    p50 = 0.020100649103767694\n",
      "    p75 = 0.020581760988373738\n",
      "    p90 = 0.020870428119137363\n",
      "    p95 = 0.020966650496058573\n",
      "    p99 = 0.02104362839759554\n",
      "    mean = 0.020100649103767694\n",
      "    min = 0.019138425334555603\n",
      "    max = 0.021062872872979782\n",
      "    stddev = 0.0013607899044574964\n",
      "ttft_s\n",
      "    p25 = 0.6060182729852386\n",
      "    p50 = 0.6837774969753809\n",
      "    p75 = 0.7615367209655233\n",
      "    p90 = 0.8081922553596087\n",
      "    p95 = 0.8237441001576371\n",
      "    p99 = 0.8361855759960599\n",
      "    mean = 0.6837774969753809\n",
      "    min = 0.5282590489950962\n",
      "    max = 0.8392959449556656\n",
      "    stddev = 0.2199362983329333\n",
      "end_to_end_latency_s\n",
      "    p25 = 3.035756326004048\n",
      "    p50 = 3.248795688006794\n",
      "    p75 = 3.4618350500095403\n",
      "    p90 = 3.589658667211188\n",
      "    p95 = 3.632266539611737\n",
      "    p99 = 3.666352837532177\n",
      "    mean = 3.248795688006794\n",
      "    min = 2.8227169640013017\n",
      "    max = 3.6748744120122865\n",
      "    stddev = 0.6025663101271902\n",
      "request_output_throughput_token_per_s\n",
      "    p25 = 48.66566561581574\n",
      "    p50 = 49.85933821606174\n",
      "    p75 = 51.05301081630773\n",
      "    p90 = 51.76921437645534\n",
      "    p95 = 52.00794889650454\n",
      "    p99 = 52.198936512543895\n",
      "    mean = 49.85933821606174\n",
      "    min = 47.47199301556974\n",
      "    max = 52.246683416553736\n",
      "    stddev = 3.3762159606021016\n",
      "number_input_tokens\n",
      "    p25 = 5999.25\n",
      "    p50 = 6013.5\n",
      "    p75 = 6027.75\n",
      "    p90 = 6036.3\n",
      "    p95 = 6039.15\n",
      "    p99 = 6041.43\n",
      "    mean = 6013.5\n",
      "    min = 5985\n",
      "    max = 6042\n",
      "    stddev = 40.30508652763321\n",
      "number_output_tokens\n",
      "    p25 = 148.5\n",
      "    p50 = 163.0\n",
      "    p75 = 177.5\n",
      "    p90 = 186.2\n",
      "    p95 = 189.1\n",
      "    p99 = 191.42\n",
      "    mean = 163.0\n",
      "    min = 134\n",
      "    max = 192\n",
      "    stddev = 41.012193308819754\n",
      "Number Of Errored Requests: 0\n",
      "Overall Output Throughput: 71.08580891278173\n",
      "Number Of Completed Requests: 2\n",
      "Completed Requests Per Minute: 26.16655542801782\n",
      "\u001b[0m\n",
      "Running benchmark with 3 concurrent requests...\n",
      "Results Directory: vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_3\n",
      "Executing Benchmark Command:\n",
      "\n",
      "    python3 ./token_benchmark_ray.py \\\n",
      "     --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
      "     --mean-input-tokens 6000 \\\n",
      "     --stddev-input-tokens 200 \\\n",
      "     --mean-output-tokens 150 \\\n",
      "     --stddev-output-tokens 50 \\\n",
      "     --max-num-completed-requests 3 \\\n",
      "     --timeout 7200 \\\n",
      "     --num-concurrent-requests 3 \\\n",
      "     --results-dir \"vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_3\" \\\n",
      "     --llm-api openai \\\n",
      "     --additional-sampling-params '{}'\n",
      "    \n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "2024-11-20 22:56:44,696\tINFO worker.py:1816 -- Started a local Ray instance.\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:05<00:00,  1.89s/it]\n",
      "\\Results for token benchmark for meta-llama/Meta-Llama-3.1-8B-Instruct queried with the openai api.\n",
      "\n",
      "inter_token_latency_s\n",
      "    p25 = 0.020016216689741684\n",
      "    p50 = 0.020987378700681934\n",
      "    p75 = 0.02267170974315659\n",
      "    p90 = 0.023682308368641387\n",
      "    p95 = 0.02401917457713632\n",
      "    p99 = 0.024288667543932264\n",
      "    mean = 0.02146282472170487\n",
      "    min = 0.01904505467880143\n",
      "    max = 0.02435604078563125\n",
      "    stddev = 0.002687225343344832\n",
      "ttft_s\n",
      "    p25 = 0.7191935630107764\n",
      "    p50 = 0.9002960990183055\n",
      "    p75 = 1.0630906395090278\n",
      "    p90 = 1.1607673638034612\n",
      "    p95 = 1.1933262719016056\n",
      "    p99 = 1.2193733983801212\n",
      "    mean = 0.8880907686737677\n",
      "    min = 0.5380910270032473\n",
      "    max = 1.22588517999975\n",
      "    stddev = 0.34405948147182147\n",
      "end_to_end_latency_s\n",
      "    p25 = 3.6679045675264206\n",
      "    p50 = 4.071805988030974\n",
      "    p75 = 4.407168742007343\n",
      "    p90 = 4.608386394393165\n",
      "    p95 = 4.6754589451884385\n",
      "    p99 = 4.729116985824658\n",
      "    mean = 4.026113543678851\n",
      "    min = 3.2640031470218673\n",
      "    max = 4.742531495983712\n",
      "    stddev = 0.740322476542563\n",
      "request_output_throughput_token_per_s\n",
      "    p25 = 44.34929409265034\n",
      "    p50 = 47.64470619922971\n",
      "    p75 = 50.074155561113486\n",
      "    p90 = 51.53182517824375\n",
      "    p95 = 52.01771505062051\n",
      "    p99 = 52.40642694852191\n",
      "    mean = 47.06739770276598\n",
      "    min = 41.05388198607097\n",
      "    max = 52.50360492299726\n",
      "    stddev = 5.746651430020871\n",
      "number_input_tokens\n",
      "    p25 = 6013.5\n",
      "    p50 = 6042.0\n",
      "    p75 = 6055.0\n",
      "    p90 = 6062.8\n",
      "    p95 = 6065.4\n",
      "    p99 = 6067.48\n",
      "    mean = 6031.666666666667\n",
      "    min = 5985\n",
      "    max = 6068\n",
      "    stddev = 42.45389656242797\n",
      "number_output_tokens\n",
      "    p25 = 164.0\n",
      "    p50 = 194.0\n",
      "    p75 = 221.5\n",
      "    p90 = 238.0\n",
      "    p95 = 243.5\n",
      "    p99 = 247.9\n",
      "    mean = 192.33333333333334\n",
      "    min = 134\n",
      "    max = 249\n",
      "    stddev = 57.518113089124654\n",
      "Number Of Errored Requests: 0\n",
      "Overall Output Throughput: 101.88630935837023\n",
      "Number Of Completed Requests: 3\n",
      "Completed Requests Per Minute: 31.78429061439626\n",
      "\u001b[0m\n",
      "Running benchmark with 4 concurrent requests...\n",
      "Results Directory: vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_4\n",
      "Executing Benchmark Command:\n",
      "\n",
      "    python3 ./token_benchmark_ray.py \\\n",
      "     --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
      "     --mean-input-tokens 6000 \\\n",
      "     --stddev-input-tokens 200 \\\n",
      "     --mean-output-tokens 150 \\\n",
      "     --stddev-output-tokens 50 \\\n",
      "     --max-num-completed-requests 4 \\\n",
      "     --timeout 7200 \\\n",
      "     --num-concurrent-requests 4 \\\n",
      "     --results-dir \"vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_4\" \\\n",
      "     --llm-api openai \\\n",
      "     --additional-sampling-params '{}'\n",
      "    \n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "2024-11-20 22:56:58,066\tINFO worker.py:1816 -- Started a local Ray instance.\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.50s/it]\n",
      "\\Results for token benchmark for meta-llama/Meta-Llama-3.1-8B-Instruct queried with the openai api.\n",
      "\n",
      "inter_token_latency_s\n",
      "    p25 = 0.02202663397041257\n",
      "    p50 = 0.024704979734564113\n",
      "    p75 = 0.02707859074954888\n",
      "    p90 = 0.027525591322240514\n",
      "    p95 = 0.027674591513137725\n",
      "    p99 = 0.027793791665855494\n",
      "    mean = 0.024400244985397335\n",
      "    min = 0.02036742876842618\n",
      "    max = 0.027823591704034936\n",
      "    stddev = 0.0035214695641683875\n",
      "ttft_s\n",
      "    p25 = 0.7716123367717955\n",
      "    p50 = 1.017723131517414\n",
      "    p75 = 1.2618171827489277\n",
      "    p90 = 1.4038818079105113\n",
      "    p95 = 1.4512366829643724\n",
      "    p99 = 1.4891205830074614\n",
      "    mean = 1.0157063880033093\n",
      "    min = 0.5287877309601754\n",
      "    max = 1.4985915580182336\n",
      "    stddev = 0.41826537391825236\n",
      "end_to_end_latency_s\n",
      "    p25 = 3.541357799738762\n",
      "    p50 = 4.026244083506754\n",
      "    p75 = 4.575571055785986\n",
      "    p90 = 4.8855599903268745\n",
      "    p95 = 4.9888896351738365\n",
      "    p99 = 5.071553351051406\n",
      "    mean = 4.090684772017994\n",
      "    min = 3.218031641037669\n",
      "    max = 5.0922192800207995\n",
      "    stddev = 0.8281153946320994\n",
      "request_output_throughput_token_per_s\n",
      "    p25 = 36.885876303132406\n",
      "    p50 = 40.776769954806504\n",
      "    p75 = 45.48693834796493\n",
      "    p90 = 47.6514801178905\n",
      "    p95 = 48.37299404119902\n",
      "    p99 = 48.95020517984584\n",
      "    mean = 41.59604469629083\n",
      "    min = 35.73613091104279\n",
      "    max = 49.09450796450754\n",
      "    stddev = 6.232043877792983\n",
      "number_input_tokens\n",
      "    p25 = 5982.5\n",
      "    p50 = 6013.5\n",
      "    p75 = 6048.5\n",
      "    p90 = 6060.2\n",
      "    p95 = 6064.1\n",
      "    p99 = 6067.22\n",
      "    mean = 6017.5\n",
      "    min = 5975\n",
      "    max = 6068\n",
      "    stddev = 44.76978147515725\n",
      "number_output_tokens\n",
      "    p25 = 130.75\n",
      "    p50 = 165.5\n",
      "    p75 = 208.75\n",
      "    p90 = 233.5\n",
      "    p95 = 241.74999999999997\n",
      "    p99 = 248.35\n",
      "    mean = 174.0\n",
      "    min = 115\n",
      "    max = 250\n",
      "    stddev = 60.94259594077036\n",
      "Number Of Errored Requests: 0\n",
      "Overall Output Throughput: 116.17109444484302\n",
      "Number Of Completed Requests: 4\n",
      "Completed Requests Per Minute: 40.05899808442863\n",
      "\u001b[0m\n",
      "Running benchmark with 5 concurrent requests...\n",
      "Results Directory: vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_5\n",
      "Executing Benchmark Command:\n",
      "\n",
      "    python3 ./token_benchmark_ray.py \\\n",
      "     --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
      "     --mean-input-tokens 6000 \\\n",
      "     --stddev-input-tokens 200 \\\n",
      "     --mean-output-tokens 150 \\\n",
      "     --stddev-output-tokens 50 \\\n",
      "     --max-num-completed-requests 5 \\\n",
      "     --timeout 7200 \\\n",
      "     --num-concurrent-requests 5 \\\n",
      "     --results-dir \"vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_5\" \\\n",
      "     --llm-api openai \\\n",
      "     --additional-sampling-params '{}'\n",
      "    \n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "2024-11-20 22:57:11,803\tINFO worker.py:1816 -- Started a local Ray instance.\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:06<00:00,  1.25s/it]\n",
      "\\Results for token benchmark for meta-llama/Meta-Llama-3.1-8B-Instruct queried with the openai api.\n",
      "\n",
      "inter_token_latency_s\n",
      "    p25 = 0.022547610402161387\n",
      "    p50 = 0.024492789802402066\n",
      "    p75 = 0.029040271872411302\n",
      "    p90 = 0.02923252008568604\n",
      "    p95 = 0.029296602823444283\n",
      "    p99 = 0.02934786901365088\n",
      "    mean = 0.025564156005937023\n",
      "    min = 0.022379422391507836\n",
      "    max = 0.02936068556120253\n",
      "    stddev = 0.0034237011879605147\n",
      "ttft_s\n",
      "    p25 = 0.7519808720098808\n",
      "    p50 = 1.0931741449749097\n",
      "    p75 = 1.4311739290133119\n",
      "    p90 = 1.5976296165958046\n",
      "    p95 = 1.653114845789969\n",
      "    p99 = 1.6975030291453004\n",
      "    mean = 1.1029585563926958\n",
      "    min = 0.5298637609812431\n",
      "    max = 1.7086000749841332\n",
      "    stddev = 0.48120829534502063\n",
      "end_to_end_latency_s\n",
      "    p25 = 3.891644517017994\n",
      "    p50 = 4.72741567902267\n",
      "    p75 = 4.848075482004788\n",
      "    p90 = 5.175506487803068\n",
      "    p95 = 5.284650156402495\n",
      "    p99 = 5.3719650912820365\n",
      "    mean = 4.459292749210727\n",
      "    min = 3.435534243006259\n",
      "    max = 5.3937938250019215\n",
      "    stddev = 0.7852770488818256\n",
      "request_output_throughput_token_per_s\n",
      "    p25 = 34.43274415585076\n",
      "    p50 = 40.82568851654276\n",
      "    p75 = 44.34749434039189\n",
      "    p90 = 44.54758640147314\n",
      "    p95 = 44.61428375516689\n",
      "    p99 = 44.667641638121886\n",
      "    mean = 39.668549362690584\n",
      "    min = 34.055838691806876\n",
      "    max = 44.680981108860635\n",
      "    stddev = 5.17861338523089\n",
      "number_input_tokens\n",
      "    p25 = 5975.0\n",
      "    p50 = 5985.0\n",
      "    p75 = 6042.0\n",
      "    p90 = 6057.6\n",
      "    p95 = 6062.8\n",
      "    p99 = 6066.96\n",
      "    mean = 5996.2\n",
      "    min = 5911\n",
      "    max = 6068\n",
      "    stddev = 61.414167746538745\n",
      "number_output_tokens\n",
      "    p25 = 134.0\n",
      "    p50 = 193.0\n",
      "    p75 = 215.0\n",
      "    p90 = 230.6\n",
      "    p95 = 235.79999999999998\n",
      "    p99 = 239.96\n",
      "    mean = 180.0\n",
      "    min = 117\n",
      "    max = 241\n",
      "    stddev = 52.91502622129181\n",
      "Number Of Errored Requests: 0\n",
      "Overall Output Throughput: 143.9940133505066\n",
      "Number Of Completed Requests: 5\n",
      "Completed Requests Per Minute: 47.99800445016887\n",
      "\u001b[0m\n",
      "Running benchmark with 6 concurrent requests...\n",
      "Results Directory: vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_6\n",
      "Executing Benchmark Command:\n",
      "\n",
      "    python3 ./token_benchmark_ray.py \\\n",
      "     --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
      "     --mean-input-tokens 6000 \\\n",
      "     --stddev-input-tokens 200 \\\n",
      "     --mean-output-tokens 150 \\\n",
      "     --stddev-output-tokens 50 \\\n",
      "     --max-num-completed-requests 6 \\\n",
      "     --timeout 7200 \\\n",
      "     --num-concurrent-requests 6 \\\n",
      "     --results-dir \"vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_6\" \\\n",
      "     --llm-api openai \\\n",
      "     --additional-sampling-params '{}'\n",
      "    \n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "2024-11-20 22:57:26,151\tINFO worker.py:1816 -- Started a local Ray instance.\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:06<00:00,  1.11s/it]\n",
      "\\Results for token benchmark for meta-llama/Meta-Llama-3.1-8B-Instruct queried with the openai api.\n",
      "\n",
      "inter_token_latency_s\n",
      "    p25 = 0.025255795402618835\n",
      "    p50 = 0.0286956280226821\n",
      "    p75 = 0.03146008264648758\n",
      "    p90 = 0.03390309659316809\n",
      "    p95 = 0.03512201438582874\n",
      "    p99 = 0.03609714861995726\n",
      "    mean = 0.028833585650337324\n",
      "    min = 0.022778573627816514\n",
      "    max = 0.03634093217848939\n",
      "    stddev = 0.005091206224501002\n",
      "ttft_s\n",
      "    p25 = 0.7875750782841351\n",
      "    p50 = 1.172165882482659\n",
      "    p75 = 1.5648549737525173\n",
      "    p90 = 1.8198083955212496\n",
      "    p95 = 1.9064075082715135\n",
      "    p99 = 1.9756867984717246\n",
      "    mean = 1.2036757331710153\n",
      "    min = 0.5295865319785662\n",
      "    max = 1.9930066210217774\n",
      "    stddev = 0.5593323553923615\n",
      "end_to_end_latency_s\n",
      "    p25 = 3.8607645190058975\n",
      "    p50 = 4.573440972511889\n",
      "    p75 = 5.144259191234596\n",
      "    p90 = 5.455260948016075\n",
      "    p95 = 5.575104439034476\n",
      "    p99 = 5.670979231849197\n",
      "    mean = 4.535969873841775\n",
      "    min = 3.4162517359945923\n",
      "    max = 5.6949479300528765\n",
      "    stddev = 0.8884272598549549\n",
      "request_output_throughput_token_per_s\n",
      "    p25 = 31.78450126513811\n",
      "    p50 = 35.169104411491276\n",
      "    p75 = 39.60226603226543\n",
      "    p90 = 41.92771887354773\n",
      "    p95 = 42.91313825991482\n",
      "    p99 = 43.70147376900849\n",
      "    mean = 35.58143165880247\n",
      "    min = 27.515536694673134\n",
      "    max = 43.8985576462819\n",
      "    stddev = 6.178638092183141\n",
      "number_input_tokens\n",
      "    p25 = 5953.25\n",
      "    p50 = 5980.0\n",
      "    p75 = 6027.75\n",
      "    p90 = 6055.0\n",
      "    p95 = 6061.5\n",
      "    p99 = 6066.7\n",
      "    mean = 5987.833333333333\n",
      "    min = 5911\n",
      "    max = 6068\n",
      "    stddev = 58.62905991627928\n",
      "number_output_tokens\n",
      "    p25 = 122.75\n",
      "    p50 = 165.5\n",
      "    p75 = 200.0\n",
      "    p90 = 225.5\n",
      "    p95 = 237.75\n",
      "    p99 = 247.55\n",
      "    mean = 165.83333333333334\n",
      "    min = 94\n",
      "    max = 250\n",
      "    stddev = 59.42866872702658\n",
      "Number Of Errored Requests: 0\n",
      "Overall Output Throughput: 149.44149166021583\n",
      "Number Of Completed Requests: 6\n",
      "Completed Requests Per Minute: 54.06928341475145\n",
      "\u001b[0m\n",
      "Running benchmark with 7 concurrent requests...\n",
      "Results Directory: vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_7\n",
      "Executing Benchmark Command:\n",
      "\n",
      "    python3 ./token_benchmark_ray.py \\\n",
      "     --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
      "     --mean-input-tokens 6000 \\\n",
      "     --stddev-input-tokens 200 \\\n",
      "     --mean-output-tokens 150 \\\n",
      "     --stddev-output-tokens 50 \\\n",
      "     --max-num-completed-requests 7 \\\n",
      "     --timeout 7200 \\\n",
      "     --num-concurrent-requests 7 \\\n",
      "     --results-dir \"vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_7\" \\\n",
      "     --llm-api openai \\\n",
      "     --additional-sampling-params '{}'\n",
      "    \n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "2024-11-20 22:57:40,813\tINFO worker.py:1816 -- Started a local Ray instance.\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:06<00:00,  1.09it/s]\n",
      "\\Results for token benchmark for meta-llama/Meta-Llama-3.1-8B-Instruct queried with the openai api.\n",
      "\n",
      "inter_token_latency_s\n",
      "    p25 = 0.03139256760195779\n",
      "    p50 = 0.0358842494781129\n",
      "    p75 = 0.039399394055978365\n",
      "    p90 = 0.15679075119686078\n",
      "    p95 = 0.24407508053476779\n",
      "    p99 = 0.3139025440050937\n",
      "    mean = 0.07658245682628621\n",
      "    min = 0.027249615117343085\n",
      "    max = 0.3313594098726753\n",
      "    stddev = 0.11244783840924802\n",
      "ttft_s\n",
      "    p25 = 0.9211467114801053\n",
      "    p50 = 1.3908639139845036\n",
      "    p75 = 1.9228585230011959\n",
      "    p90 = 2.2493853108142505\n",
      "    p95 = 2.359139639424393\n",
      "    p99 = 2.446943102312507\n",
      "    mean = 1.44009516128738\n",
      "    min = 0.5328977780300193\n",
      "    max = 2.4688939680345356\n",
      "    stddev = 0.7078321879196723\n",
      "end_to_end_latency_s\n",
      "    p25 = 3.973535928496858\n",
      "    p50 = 4.306385402043816\n",
      "    p75 = 4.963275721500395\n",
      "    p90 = 5.466408727609087\n",
      "    p95 = 5.5193606038112195\n",
      "    p99 = 5.561722104772925\n",
      "    mean = 4.3460332107164765\n",
      "    min = 2.6699112929636613\n",
      "    max = 5.572312480013352\n",
      "    stddev = 0.9801646796098935\n",
      "request_output_throughput_token_per_s\n",
      "    p25 = 25.396132631610225\n",
      "    p50 = 27.86559696748179\n",
      "    p75 = 32.08499185455001\n",
      "    p90 = 35.567059665011165\n",
      "    p95 = 36.13111235369552\n",
      "    p99 = 36.58235450464301\n",
      "    mean = 26.074195030504548\n",
      "    min = 2.9963542313496943\n",
      "    max = 36.69516504237988\n",
      "    stddev = 11.08706467602602\n",
      "number_input_tokens\n",
      "    p25 = 5928.5\n",
      "    p50 = 5975.0\n",
      "    p75 = 6013.5\n",
      "    p90 = 6052.4\n",
      "    p95 = 6060.2\n",
      "    p99 = 6066.44\n",
      "    mean = 5974.714285714285\n",
      "    min = 5896\n",
      "    max = 6068\n",
      "    stddev = 63.79057998825607\n",
      "number_output_tokens\n",
      "    p25 = 101.0\n",
      "    p50 = 120.0\n",
      "    p75 = 163.5\n",
      "    p90 = 195.6\n",
      "    p95 = 196.8\n",
      "    p99 = 197.76\n",
      "    mean = 122.14285714285714\n",
      "    min = 8\n",
      "    max = 198\n",
      "    stddev = 64.59470713979738\n",
      "Number Of Errored Requests: 0\n",
      "Overall Output Throughput: 132.8234220522346\n",
      "Number Of Completed Requests: 7\n",
      "Completed Requests Per Minute: 65.246593288817\n",
      "\u001b[0m\n",
      "Running benchmark with 8 concurrent requests...\n",
      "Results Directory: vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_8\n",
      "Executing Benchmark Command:\n",
      "\n",
      "    python3 ./token_benchmark_ray.py \\\n",
      "     --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
      "     --mean-input-tokens 6000 \\\n",
      "     --stddev-input-tokens 200 \\\n",
      "     --mean-output-tokens 150 \\\n",
      "     --stddev-output-tokens 50 \\\n",
      "     --max-num-completed-requests 8 \\\n",
      "     --timeout 7200 \\\n",
      "     --num-concurrent-requests 8 \\\n",
      "     --results-dir \"vllm_bench_results/zoom-tp8-b8/2024-11-20-22-56-16/concurrent_8\" \\\n",
      "     --llm-api openai \\\n",
      "     --additional-sampling-params '{}'\n",
      "    \n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "2024-11-20 22:57:55,426\tINFO worker.py:1816 -- Started a local Ray instance.\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:35<00:00,  4.40s/it]\n",
      "\\Results for token benchmark for meta-llama/Meta-Llama-3.1-8B-Instruct queried with the openai api.\n",
      "\n",
      "inter_token_latency_s\n",
      "    p25 = 0.17167360213732957\n",
      "    p50 = 0.23743288335341206\n",
      "    p75 = 0.293758030003813\n",
      "    p90 = 0.30503751648643507\n",
      "    p95 = 0.31681745308715803\n",
      "    p99 = 0.32624140236773647\n",
      "    mean = 0.23396183032988668\n",
      "    min = 0.1366646678915713\n",
      "    max = 0.32859738968788105\n",
      "    stddev = 0.06934639818603341\n",
      "ttft_s\n",
      "    p25 = 1.1524430202553049\n",
      "    p50 = 1.7486274445254821\n",
      "    p75 = 2.368429705777089\n",
      "    p90 = 2.7007422202150337\n",
      "    p95 = 2.79719508861308\n",
      "    p99 = 2.8743573833315166\n",
      "    mean = 1.743619872009731\n",
      "    min = 0.5300551019608974\n",
      "    max = 2.893647957011126\n",
      "    stddev = 0.837683525558795\n",
      "end_to_end_latency_s\n",
      "    p25 = 32.240004752995446\n",
      "    p50 = 32.59257899300428\n",
      "    p75 = 33.46013237623265\n",
      "    p90 = 33.6894513030129\n",
      "    p95 = 33.92803218051267\n",
      "    p99 = 34.11889688251249\n",
      "    mean = 32.78170619950106\n",
      "    min = 31.54586420900887\n",
      "    max = 34.166613058012445\n",
      "    stddev = 0.8561265800399331\n",
      "request_output_throughput_token_per_s\n",
      "    p25 = 3.404142945589376\n",
      "    p50 = 4.219099434933055\n",
      "    p75 = 5.82495149814524\n",
      "    p90 = 6.275616145700576\n",
      "    p95 = 6.796350398592644\n",
      "    p99 = 7.212937800906298\n",
      "    mean = 4.6563044820950115\n",
      "    min = 3.0431881454870497\n",
      "    max = 7.317084651484712\n",
      "    stddev = 1.5129879508628856\n",
      "number_input_tokens\n",
      "    p25 = 5937.25\n",
      "    p50 = 5980.0\n",
      "    p75 = 6048.5\n",
      "    p90 = 6214.4\n",
      "    p95 = 6385.2\n",
      "    p99 = 6521.84\n",
      "    mean = 6047.375\n",
      "    min = 5896\n",
      "    max = 6556\n",
      "    stddev = 213.8330039339778\n",
      "number_output_tokens\n",
      "    p25 = 109.75\n",
      "    p50 = 137.5\n",
      "    p75 = 195.0\n",
      "    p90 = 211.5\n",
      "    p95 = 230.74999999999997\n",
      "    p99 = 246.14999999999998\n",
      "    mean = 153.75\n",
      "    min = 96\n",
      "    max = 250\n",
      "    stddev = 54.126175341906965\n",
      "Number Of Errored Requests: 0\n",
      "Overall Output Throughput: 34.928613393175354\n",
      "Number Of Completed Requests: 8\n",
      "Completed Requests Per Minute: 13.630678397336723\n",
      "\u001b[0m\n",
      "All Benchmark Results:\n",
      "total 32\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4096 Nov 20 22:56 concurrent_1\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4096 Nov 20 22:56 concurrent_2\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4096 Nov 20 22:56 concurrent_3\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4096 Nov 20 22:57 concurrent_4\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4096 Nov 20 22:57 concurrent_5\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4096 Nov 20 22:57 concurrent_6\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4096 Nov 20 22:57 concurrent_7\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4096 Nov 20 22:58 concurrent_8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# User Variables - Modify these as needed\n",
    "\n",
    "# Maximum number of concurrent requests to test\n",
    "max_concurrent_requests = 8\n",
    "\n",
    "# IP address of the remote server\n",
    "remote_ip = '3.142.73.135'\n",
    "\n",
    "# Model to use for benchmarking\n",
    "model = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "\n",
    "# Mean and standard deviation of input tokens\n",
    "mean_input_tokens = 6000\n",
    "stddev_input_tokens = 200\n",
    "\n",
    "# Mean and standard deviation of output tokens\n",
    "mean_output_tokens = 150\n",
    "stddev_output_tokens = 50\n",
    "\n",
    "# Absolute path to clone the llmperf repository\n",
    "llmperf_dir = '/home/ec2-user/SageMaker/llmperf'\n",
    "\n",
    "# Maximum time to wait for each benchmark run (in seconds)\n",
    "timeout = 7200\n",
    "\n",
    "# Additional sampling parameters for the benchmark\n",
    "additional_sampling_params = '{}'\n",
    "\n",
    "# Clone the llmperf repository if it doesn't exist\n",
    "if not os.path.exists(llmperf_dir):\n",
    "    !git clone https://github.com/ray-project/llmperf.git {llmperf_dir}\n",
    "else:\n",
    "    print(f\"'llmperf' directory already exists at {llmperf_dir}\")\n",
    "\n",
    "# Change the current working directory to the llmperf directory\n",
    "os.chdir(llmperf_dir)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Install required packages\n",
    "!pip install setuptools==65.5.0\n",
    "!pip install -e .\n",
    "\n",
    "# Set environment variables for the OpenAI API\n",
    "os.environ['OPENAI_API_KEY'] = 'EMPTY'\n",
    "os.environ['OPENAI_API_BASE'] = f\"http://{remote_ip}:8000/v1\"\n",
    "\n",
    "# Generate a timestamped directory name for results\n",
    "date_str = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "# Modify this to change the results directory\n",
    "base_results_dir = f\"vllm_bench_results/zoom-tp8-b8/{date_str}\"\n",
    "\n",
    "print(f\"Base Results Directory: {base_results_dir}\")\n",
    "\n",
    "# Create the base results directory\n",
    "os.makedirs(base_results_dir, exist_ok=True)\n",
    "\n",
    "# Run benchmarks for concurrent requests from 1 to max_concurrent_requests\n",
    "for concurrent_requests in range(1, max_concurrent_requests + 1):\n",
    "    print(f\"\\nRunning benchmark with {concurrent_requests} concurrent requests...\")\n",
    "    \n",
    "    # Compute the maximum number of requests for this run\n",
    "    max_requests = concurrent_requests * 1\n",
    "    \n",
    "    # Create a subdirectory for this run's results\n",
    "    results_dir = os.path.join(base_results_dir, f\"concurrent_{concurrent_requests}\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    print(f\"Results Directory: {results_dir}\")\n",
    "    \n",
    "    # Construct the benchmark command\n",
    "    benchmark_command = f\"\"\"\n",
    "    python3 ./token_benchmark_ray.py \\\\\n",
    "     --model {model} \\\\\n",
    "     --mean-input-tokens {mean_input_tokens} \\\\\n",
    "     --stddev-input-tokens {stddev_input_tokens} \\\\\n",
    "     --mean-output-tokens {mean_output_tokens} \\\\\n",
    "     --stddev-output-tokens {stddev_output_tokens} \\\\\n",
    "     --max-num-completed-requests {max_requests} \\\\\n",
    "     --timeout {timeout} \\\\\n",
    "     --num-concurrent-requests {concurrent_requests} \\\\\n",
    "     --results-dir \"{results_dir}\" \\\\\n",
    "     --llm-api openai \\\\\n",
    "     --additional-sampling-params '{additional_sampling_params}'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Print the command for verification\n",
    "    print(\"Executing Benchmark Command:\")\n",
    "    print(benchmark_command)\n",
    "    \n",
    "    # Execute the benchmark command\n",
    "    !{benchmark_command}\n",
    "    \n",
    "# List the contents of the base results directory\n",
    "print(\"\\nAll Benchmark Results:\")\n",
    "!ls -l \"{base_results_dir}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721b768-ad73-4c0d-8134-e23cf92fe517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
